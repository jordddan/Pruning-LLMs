{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amax/anaconda3/envs/megatron/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoModelForCausalLM, LlamaTokenizerFast\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.82s/it]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15258\n",
      "tensor([[    1,  1516,   337,   277,  2493,   309,   277,  9490, 15246]],\n",
      "       device='cuda:5')\n",
      "['<s>What is the color of the sky?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model1 = LlamaForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"/nvme/qd/ckpt/prune_llama2\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    local_files_only=True,\n",
    "    use_safetensors=False\n",
    ").to(\"cuda:5\")\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"/nvme/qd/ckpt/prune_llama2\",use_fast=False)\n",
    "prompt1 = f\"\"\"What is the color of the sky?\"\"\"\n",
    "input_ids1 = tokenizer1(prompt1, return_tensors=\"pt\", truncation=True).input_ids.to(model1.device)\n",
    "print(tokenizer1.sp_model.vocab_size())\n",
    "print(input_ids1)\n",
    "print(tokenizer1.batch_decode(input_ids1.detach().cpu().numpy()))\n",
    "\n",
    "# outputs1 = model1.generate(input_ids=input_ids1, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "# print(f\"Generated instruction:\\n{tokenizer1.batch_decode(outputs1, skip_special_tokens=True)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1516,   337,   277,  2493,   309,   277,  9490, 15246, 10713,\n",
      "         11781,  6505,  4692, 12993, 12530,  9883, 11607,  6163,  3435, 12791,\n",
      "          6473, 10041,  8222,  6557,  4542, 13688,  6508,  7566,  8899, 11812,\n",
      "         13957, 10188,  5609, 12241,  4079,  8041,  5145,  9123,  9820, 13632,\n",
      "          3552,  2482,  5406,  9547,  9815, 14900,  2570,  8298, 13387, 11695,\n",
      "          6758,  3002,  3278,  5238,  3874,  6124, 11863, 13536, 12312,  9098,\n",
      "         12517,  7105,  6616, 12032,  7689, 10623, 14795, 13819, 13640, 12441,\n",
      "          5940,  5373,  4250,  7454,  5904,  4325,  1932,  8887,  6773,  8562,\n",
      "         12865,  4969, 14077,  7315, 11117, 11748,  6435,  8581, 10163,  3583,\n",
      "         14056, 12141,  4072,  2377,  3266,  5645, 14896,  7188,  9070, 15194,\n",
      "          7091,  4185, 13914,  8166, 11210,  3609, 14698,  1851, 10340]],\n",
      "       device='cuda:5')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.80s/it]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "tensor([[    1,  1724,   338,   278,  2927,   310,   278, 14744, 29973]],\n",
      "       device='cuda:5')\n",
      "['<s>What is the color of the sky?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model2 = LlamaForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"/nvme/hf_models/Llama-2-7b-hf\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    local_files_only=True,\n",
    "    use_safetensors=False\n",
    ").to(\"cuda:5\")\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"/nvme/hf_models/Llama-2-7b-hf\",use_fast=False)\n",
    "prompt2 = f\"\"\"What is the color of the sky?\"\"\"\n",
    "input_ids2 = tokenizer2(prompt2, return_tensors=\"pt\", truncation=True).input_ids.to(model1.device)\n",
    "print(tokenizer2.sp_model.vocab_size())\n",
    "print(input_ids2)\n",
    "print(tokenizer2.batch_decode(input_ids2.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = model1(input_ids1).logits\n",
    "output2 = model2(input_ids2).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.9922, -6.9922, -6.9922,  ..., -6.9922, -6.9922, -6.9922],\n",
      "         [-6.9922, -6.9922, -6.9922,  ..., -6.9922, -6.9922, -6.9922],\n",
      "         [-6.9922, -6.9922, -6.9922,  ..., -6.9922, -6.9922, -6.9922],\n",
      "         ...,\n",
      "         [-6.9922, -6.9922, -6.9922,  ..., -6.9922, -6.9922, -6.9922],\n",
      "         [-6.9922, -6.9922, -6.9922,  ..., -6.9922, -6.9922, -6.9922],\n",
      "         [-6.9922, -6.9922, -6.9922,  ..., -6.9922, -6.9922, -6.9922]]],\n",
      "       device='cuda:5', grad_fn=<ToCopyBackward0>)\n",
      "tensor([[[-12.8281,  -7.4453,  -0.4639,  ...,  -6.7812,  -8.0156,  -7.5039],\n",
      "         [-12.6016,  -9.4531,  -2.6113,  ...,  -7.0469,  -9.5156,  -9.2422],\n",
      "         [-13.0625, -15.0000,  -3.6855,  ...,  -6.0625,  -8.1484,  -8.5078],\n",
      "         ...,\n",
      "         [ -9.8125,  -9.0703,   0.5933,  ...,  -6.9336,  -6.3359,  -6.5039],\n",
      "         [ -4.5469,  -3.0176,   9.6094,  ...,  -2.8027,  -3.2480,  -3.2344],\n",
      "         [-12.0703, -10.5781,   6.9219,  ...,  -5.9609,  -7.5508,  -5.8828]]],\n",
      "       device='cuda:5', grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output1)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1516,   337,   277,  2493,   309,   277,  9490, 15246]],\n",
      "       device='cuda:5')\n",
      "tensor([[    1,  1724,   338,   278,  2927,   310,   278, 14744, 29973]],\n",
      "       device='cuda:5')\n",
      "Parameter containing:\n",
      "tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        ...,\n",
      "        [ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07]], device='cuda:5', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        ...,\n",
      "        [ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07]], device='cuda:5', dtype=torch.float16,\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(input_ids1)\n",
    "print(input_ids2)\n",
    "print(model1.model.embed_tokens.weight[1].sum())\n",
    "print(model2.model.embed_tokens.weight[1].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp_model = spm.SentencePieceProcessor(model_file=\"/nvme/qd/ckpt/prune_llama2/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15258\n",
      "[359]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(sp_model.vocab_size())\n",
    "print(sp_model.Encode(\"D\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megatron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
